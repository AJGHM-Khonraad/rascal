Name: Crawl

Synopsis: Crawl a file system and collect files with given extension

Syntax:

Types:

Function:

Details:

Description:

A [Web Crawler](http://en.wikipedia.org/wiki/Web_crawler) is a program that browses web pages in a systematic manner.
Here we present a crawler for files in a file system, although it can easily be extended to browse the web as well.

Examples:

<listing demo/common/Crawl.rsc>

The `crawl` function takes two arguments:
* The directory `dir` where the crawl should start.
* The suffix of the files we are looking for.


The crawler simply lists all entries in the current directory using [Rascal:listEntries] and adds all files
that end we the given suffix to the result. If an entry is itself a directory, it is crawled by a recursive call to crawl.
At /*1*/ we use concatenation on a location and a string; it automatically adds a `/` separator, see [Rascal:Location] for details.

Let's now crawl the demo files used in this course:

<screen>
import demo::common::Crawl;
files = crawl(|std://library/demo|, ".rsc");
// This list can be the starting point for further investigation, for instance, to compute the number of files;
import List;
size(files);
// or to compute the total number of lines in these files:
import IO;
(0 | it + size(readFileLines(f)) | loc f <- files);
// This above example uses a [$Rascal:Expressions/Reducer] expression.
//
// Finally, we can look for the largest files. First import some libraries we will need.
import Relation;
import Set;
// Create a relation `aux` of `<location, length>` pairs:
aux = {<f, size(readFileLines(f))> | loc f <- files};
// Now determine the maximum length (using [$Rascal:Set/size]), by taking the maximum of the [$Rascal:Relation/range] of `aux`:
m = max(range(aux));
// and now [$Rascal:Relation/invert] `aux` and use [$Rascal:Relation/Subscription]  to find the tuples with the maximum value as first element.
invert(aux)[m];
</screen>

Benefits:

Pitfalls:

Questions:

       
